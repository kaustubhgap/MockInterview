{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q openai\n",
    "!pip install -qU langchain-openai\n",
    "!pip install -q langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    }
   ],
   "source": [
    "print(\"here\")\n",
    "\"kk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course! I'd be happy to conduct a mock interview with you. To get started, could you please provide me with a few details?\n",
      "\n",
      "1. What type of position or field are you interviewing for (e.g., software engineering, marketing, management)?\n",
      "2. Do you have any specific areas you would like to focus on (e.g., technical questions, behavioral questions, problem-solving)?\n",
      "3. Would you prefer a formal structure or a more casual conversation style?\n",
      "\n",
      "Feel free to share any other information you think might be relevant!"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Can you take a mock interview for me?\"}],\n",
    "    stream=True,\n",
    ")\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain import LLMChain\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bots.job_descriptions import datascience as jd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi Bob! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 11, 'total_tokens': 21, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-8c4bd27d-8e9d-4335-8bd2-a72c83ffd993-0', usage_metadata={'input_tokens': 11, 'output_tokens': 10, 'total_tokens': 21})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.invoke([HumanMessage(content=\"Hi! I'm Bob\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bots.InterviewBot import DataScience, Interviewer\n",
    "from Bots.AnswerBot import AnsweringBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(interviewBot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = DataScience.PromptInstructionType1()\n",
    "interviewer = Interviewer.FirstInterviewer()\n",
    "answerer = AnsweringBot.AnsweringBot1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\DSMLAI\\llm\\LLMExperiments\\Bots\\InterviewBot.py:17: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  response = LLMChain(\n"
     ]
    }
   ],
   "source": [
    "topics_output = topics(llm=model, job_description=jd.jd1['long'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-AI and ML Solution Design-Implementation Based-8  \n",
      "2-Model Deployment and Maintenance-Theoretical-7  \n",
      "3-Data Preprocessing and Feature Engineering-Implementation Based-8  \n",
      "4-Algorithm Selection and Evaluation-Theoretical-7  \n",
      "5-Cloud Platforms for AI Development-Theoretical-6  \n"
     ]
    }
   ],
   "source": [
    "print(topics_output['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_response = topics.extract_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"feedback\":None,\n",
      "\"question_type\":\"new\",\n",
      "\"current_question_topic\":\"AI and ML Solution Design\",\n",
      "\"question\":\"Can you describe a specific AI or ML solution you designed and implemented? What were the key challenges you faced during the design process, and how did you overcome them?\"}\n"
     ]
    }
   ],
   "source": [
    "interviewer_output = interviewer(llm=model, \n",
    "                                 job_description=jd.jd1['long'], \n",
    "                                 topics=topics_output['text'].replace(\"-\",\"|\"),\n",
    "                                 current_topic=topics_response[0][1],\n",
    "                                 question_history=\"None\",\n",
    "                                 previous_question=\"None\",\n",
    "                                 previous_answer=\"None\")\n",
    "\n",
    "print(interviewer_output['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = \"\"\"Absolutely! One of the most impactful projects I worked on was a customer sentiment analysis tool for a retail company.\n",
    "They were struggling to understand customer feedback coming from various channels like social media, product reviews, and support tickets.\n",
    " This made it really hard for them to identify areas for improvement and respond effectively to their customers.\n",
    " In my role as the lead data scientist, I started by collaborating with stakeholders to figure out exactly what insights they needed.\n",
    " We wanted to get a clear picture of overall sentiment and common themes in customer feedback. Once we nailed down the requirements, I moved on to data collection.\n",
    "I gathered data from multiple sources using APIs and some web scraping, ensuring we had a diverse dataset that represented different customer demographics. \n",
    "Next, I focused on cleaning and preprocessing the text data. This involved tasks like tokenization and removing stop words, which helped prepare the data for analysis. \n",
    "After that, I evaluated various NLP models and decided to implement a combination of traditional machine learning algorithms and deep learning approaches for sentiment classification.\n",
    "Once I trained the models using labeled datasets, I made sure to validate them through cross-validation techniques to ensure they were robust. After that, I built an API using Flask, which allowed other teams to easily access the sentiment analysis results. To visualize the insights, I developed a dashboard using Tableau, making it straightforward for stakeholders to interpret the data. Finally, I set up monitoring to keep track of the modelâ€™s performance and gathered feedback from users to enable continuous improvement. The outcome was really positive; the tool provided real-time insights that helped the company make data-driven decisions about product features and marketing strategies, ultimately boosting customer satisfaction scores. I found the whole experience incredibly rewarding because it showed me how powerful data can be in driving business success. My technical skills and ability to collaborate with cross-functional teams were key to making this project a success.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"feedback\":\"- Strong understanding of NLP and sentiment analysis.  \\n- Demonstrated ability to lead a project from requirement gathering to deployment.  \\n- Effective collaboration with stakeholders and cross-functional teams.  \\n- Experience in data collection, preprocessing, model evaluation, and API development.  \\n- Positive impact on business outcomes through data-driven decisions.  \\n- Skilled in using visualization tools like Tableau for insights delivery.\",\"question_type\":\"followup\",\"question\":\"Can you elaborate on the specific algorithms you evaluated for sentiment classification and why you chose the combination of traditional machine learning and deep learning approaches?\"}\n"
     ]
    }
   ],
   "source": [
    "interviewer_output = interviewer(llm=model, \n",
    "                                 job_description=jd.jd1['long'], \n",
    "                                 topics=topics_output['text'].replace(\"-\",\"|\"),\n",
    "                                 current_topic=topics_response[0][1],\n",
    "                                 question_history=\"None\",\n",
    "                                 previous_question=\"Can you describe a specific AI or ML solution you designed and developed? What were the business challenges it addressed, and what was your role in the project?\",\n",
    "                                 previous_answer=answer)\n",
    "\n",
    "print(interviewer_output['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interviewer_output['text']['feedback']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    topics = DataScience.PromptInstructionType1()\n",
    "    interviewer = Interviewer.FirstInterviewer()\n",
    "    answerer = AnsweringBot.AnsweringBot1()\n",
    "\n",
    "    topics_output = topics(llm=model, job_description=jd.jd1['long'])\n",
    "    topics_response = topics.extract_response()\n",
    "\n",
    "    job_description=jd.jd1['long']\n",
    "    topics=topics_output['text'].replace(\"-\",\"|\")\n",
    "    current_topic=topics_response[0][1]\n",
    "    question_history=\"None\"\n",
    "    previous_question=\"None\"\n",
    "    previous_answer=\"None\"\n",
    "\n",
    "    while True:\n",
    "        interviewer_output = interviewer(llm=model, \n",
    "                                    job_description=job_description, \n",
    "                                    topics=topics_output['text'].replace(\"-\",\"|\"),\n",
    "                                    current_topic=topics_response[0][1],\n",
    "                                    question_history=\"None\",\n",
    "                                    previous_question=\"None\",\n",
    "                                    previous_answer=\"None\")\n",
    "\n",
    "    print(interviewer_output['text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
